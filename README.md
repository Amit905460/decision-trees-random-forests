# Task 5 â€“ Decision Trees & Random Forests (AI & ML Internship)

## ðŸ“Œ Objective
Learn and apply tree-based models (Decision Trees & Random Forests) for classification tasks. Perform EDA, control overfitting, interpret feature importance, and evaluate using cross-validation.

---

## ðŸ“‚ Dataset
**Heart Disease Dataset** â€“ [Kaggle Link](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)

---

## ðŸ›  Tools Used
- Python 3.x
- Pandas, NumPy â€“ Data handling
- Matplotlib, Seaborn â€“ Data visualization
- Scikit-learn â€“ Model training & evaluation

---

## ðŸ“Š Exploratory Data Analysis (EDA)
- Target distribution plot
- Correlation heatmap
- Histograms for numeric features
- Boxplots for outlier detection

---

## ðŸ¤– Models Trained
1. **Decision Tree (Default)**
2. **Decision Tree (Pruned)** â€“ `max_depth=4`, `min_samples_split=5`, `min_samples_leaf=2`
3. **Random Forest** â€“ 100 estimators

---

## ðŸ“ˆ Results

| Model                     | Test Accuracy | CV Accuracy |
|---------------------------|--------------:|------------:|
| Decision Tree (Default)   | 0.9853         | 0.9914      |
| Decision Tree (Pruned)    | 0.8000         | 0.8390      |
| Random Forest             | 0.9853         | 0.9939      |

---

## ðŸ“Œ Key Insights
- **Random Forest** had the highest cross-validation accuracy.
- Pruning reduced overfitting but also lowered accuracy.
- Feature importance analysis revealed top predictors like `cp`, `thalach`, and `oldpeak`.

---

## ðŸ“· Visualizations Included
- Correlation heatmap
- Decision Tree plot
- Feature importance bar chart
- Model performance heatmap

---

## ðŸ“œ How to Run
```bash
pip install pandas numpy matplotlib seaborn scikit-learn
python task5_decision_tree_random_forest.py
